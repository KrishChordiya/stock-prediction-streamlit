{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1fc4c1a-fb6f-4a3b-99d0-599be50a1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c54e1b-bdb4-4ab8-9d1e-544148f92fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_analysis(df):\n",
    "    col_names = df[df.isnull().any(axis=1)]['Name'].unique()\n",
    "    for name in col_names:\n",
    "        comp_data = df[df['Name'] == name]\n",
    "        print(f'\\n---------{name}---------')\n",
    "        print(comp_data.isnull().sum() *100 / len(comp_data))\n",
    "\n",
    "def modified_null_analysis(df, col, threshold):\n",
    "    col_names = df[df.isnull().any(axis=1)]['Name'].unique()\n",
    "    comp_list = []\n",
    "    for name in col_names:\n",
    "        comp_data = df[df['Name'] == name]\n",
    "        if comp_data.isnull().sum()[col] *100 / len(comp_data) > threshold:\n",
    "            comp_list.append(name)\n",
    "    return comp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba6142df-925f-461a-b2fb-dfd4dcb6d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = './'\n",
    "df = pd.read_csv(working_dir+'preprocessed_CAC40.csv', parse_dates=['Date'])\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "drop_list = modified_null_analysis(df, 'Volume', 50)\n",
    "df = df.query(f'Name != {drop_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43b8d2b9-0d31-4862-be8d-92f2773cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy() # Saving our work till here\n",
    "df2['Volume'] = df2['Volume'].str.replace(',', '')\n",
    "df2['Volume'] = df2['Volume'].astype(float)\n",
    "trial = IterativeImputer(random_state=999, estimator=BayesianRidge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15192aa3-c33b-475f-8b02-e30435a19f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avyakta/Desktop/coding/myenv/lib/python3.11/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.DataFrame(trial.fit_transform(df2.iloc[:, 2:]))\n",
    "df3.columns = df2.iloc[:, 2:].columns\n",
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1825ae8-647b-4df7-a503-04405e7655fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5 + 1):\n",
    "    df4[f'Open(t-{i})'] = df4['Open'].shift(i)\n",
    "    df4[f'Closing_Price(t-{i})'] = df4['Closing_Price'].shift(i)\n",
    "    df4[f'Daily_High(t-{i})'] = df4['Daily_High'].shift(i)\n",
    "    df4[f'Daily_Low(t-{i})'] = df4['Daily_Low'].shift(i)\n",
    "    df4[f'Volume(t-{i})'] = df4['Volume'].shift(i)\n",
    "\n",
    "df4.dropna(inplace=True)\n",
    "df4.drop(['Open', 'Daily_High', 'Daily_Low', 'Volume'], axis=1, inplace=True)\n",
    "df4 = df4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85bf0063-cbc7-495f-b721-8b27dfac0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df4.drop('Closing_Price', axis=1).values, df4['Closing_Price'].values, test_size=0.2, random_state=999)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, X_test = X_train.reshape(-1, 5, 5), X_test.reshape(-1, 5, 5)\n",
    "X_train, X_test, y_train, y_test = torch.tensor(X_train, dtype=torch.float), torch.tensor(X_test, dtype=torch.float), torch.tensor(y_train, dtype=torch.float), torch.tensor(y_test, dtype=torch.float)\n",
    "train_set = TensorDataset(X_train, y_train)\n",
    "test_set = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47496ff6-7a39-4d77-ace7-d54603c14607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ee52877-2c64-4a7d-b622-9d11b096e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'optimizer':'Adam','n_layerlstm': 2, 'hidden_state': 82, 'n_layersfc': 2, 'n_units_l0': 91, 'dropout_l0': 0.14192353580674466, 'n_units_l1': 57, 'dropout_l1': 0.12740415994127938, 'lr': 0.0071326214304045015}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b731761c-7dbe-4457-b959-c1b8d73bd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(5, best_params['hidden_state'], batch_first=True, num_layers=best_params['n_layerlstm'])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(best_params['hidden_state'], best_params['n_units_l0']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(best_params['dropout_l0']),\n",
    "            nn.Linear(best_params['n_units_l0'], best_params['n_units_l1']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(best_params['dropout_l1']),\n",
    "            nn.Linear(best_params['n_units_l1'], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        return self.fc(hn[-1])\n",
    "\n",
    "model = MyModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa86a07-2b5d-476a-8694-fa6f54d69cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = getattr(torch.optim, best_params['optimizer'])(model.parameters(), best_params['lr'])\n",
    "loss_fn = nn.MSELoss()\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 64\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d140fe45-ddd5-4ba9-8bc8-ab54ace79ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Train Loss | Test Loss\n",
      "0 56.07621495767695 3.654884304668083 \n",
      "1 1.3144201401804314 0.26087923497572296 \n",
      "2 1.1316780632392305 0.3519186398289243 \n",
      "3 1.0421631915769616 0.47276472507930195 \n",
      "4 1.1063320232463665 1.0943430645055459 \n",
      "5 0.9888653786944561 0.11760301567369798 \n",
      "6 1.0056542081788915 0.17126675623423251 \n",
      "7 1.014162886582437 0.9576932580256071 \n",
      "8 1.0302165090182767 0.6066819783910865 \n",
      "9 0.9786084799981508 0.23727981092744185 \n",
      "10 0.9904279021821061 0.3258759190679574 \n",
      "11 0.9742587963271825 0.3337278529757359 \n",
      "12 0.9832355831368048 0.35118687458214215 \n",
      "13 0.9322684235015853 0.3184953963719919 \n",
      "14 0.93751780749833 0.3193758849787419 \n",
      "15 0.9480630930696354 0.181317804320181 \n",
      "16 0.9447283491370131 0.3709682552601959 \n",
      "17 0.9516753399225532 0.20284845438770582 \n",
      "18 0.8948523618097677 0.30771615716521855 \n",
      "19 0.8788467691753243 0.14707482997022692 \n",
      "20 0.8609523177757615 0.7204474826938794 \n",
      "21 0.922529155296869 0.5980521323250942 \n",
      "22 0.8187487834423292 0.13857758258942698 \n",
      "23 0.8930001935204033 0.4264324367290638 \n",
      "24 0.8528385353503657 0.3505634640145009 \n",
      "25 0.8848530197302338 0.1972668339177722 \n",
      "26 0.8373788164043036 0.2511565584811519 \n",
      "27 0.839098651084255 0.2267448260891633 \n",
      "28 0.8805844469576097 0.12272980221409778 \n",
      "29 0.8361683516289855 0.26733124017959736 \n",
      "30 0.777787088920347 0.4801981775242774 \n",
      "31 0.8158686500286958 0.21130756948326454 \n",
      "32 0.8104045101304035 0.214887958783351 \n",
      "33 0.8077453816095825 0.12033512984539886 \n",
      "34 0.7741774809470431 0.16691048979209583 \n",
      "35 0.7783552100423907 0.38446850280781264 \n",
      "36 0.7644815252880093 0.09354842832067707 \n",
      "37 0.7757740540460485 0.1162257922935437 \n",
      "38 0.7576807551085949 0.22061405315628793 \n",
      "39 0.7386075739489227 0.2860787985999076 \n"
     ]
    }
   ],
   "source": [
    "print('Epoch | Train Loss | Test Loss')\n",
    "for epoch in range(EPOCHS):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  for X, y in train_loader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    out = model(X).squeeze(1)\n",
    "    loss = loss_fn(out, y)\n",
    "\n",
    "    train_loss += (loss/TRAIN_BATCH_SIZE).item()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    test_loss = 0\n",
    "    for X, y in test_loader:\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      out = model(X).squeeze(1)\n",
    "      loss = loss_fn(out, y)\n",
    "      test_loss += (loss/TEST_BATCH_SIZE).item()\n",
    "\n",
    "  print(f'{epoch} {train_loss/len(train_loader)} {test_loss/len(test_loader)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b3cdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './lstmModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4ee40ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.gz']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
